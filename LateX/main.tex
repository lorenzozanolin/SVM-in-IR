% Welcome! This is the unofficial University of Udine beamer template.

% See README.md for more informations about this template.

% This style has been developed following the "Manuale di Stile"
% (Style Manual) of the University of Udine. You can find the
% manual here: https://www.uniud.it/it/ateneo-uniud/ateneo-uniud/identita-visiva/manuali-immagine-stile/manuale-stile

% Note: for some reason, the RGB values specified in the manual
% do NOT render correctly in Beamer, so they have been redefined
% for this document using the high level chromo-optic deep neural 
% quantistic technology offered by Microsoft Paint's color picker.

% We defined four theme colors: UniBrown, UniBlue, UniGold
% and UniOrange. For example, to write some uniud-brownish
% text, just use: \textcolor{UniBrown}{Hello!}

% Note that [usenames,dvipsnames] is MANDATORY due to compatibility
% issues between tikz and xcolor packages.

\documentclass[usenames,dvipsnames]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}% http://ctan.org/pkg/algorithm

\usetheme{uniud}
\graphicspath{ {./graphics/} }
\usepackage{mathtools}
%%% Bibliography
\usepackage[style=authoryear,backend=biber]{biblatex}
\addbibresource{bibliography.bib}

% Author names in publication list are consistent 
% i.e. name1 surname1, name2 surname2
% See https://tex.stackexchange.com/questions/106914/biblatex-does-not-reverse-the-first-and-last-names-of-the-second-author
\DeclareNameAlias{author}{given-family}

%%% Suppress biblatex annoying warning
\usepackage{silence}
\WarningFilter{biblatex}{Patching footnotes failed}

%%% Some useful commands
% pdf-friendly newline in links
\newcommand{\pdfnewline}{\texorpdfstring{\newline}{ }} 
% Fill the vertical space in a slide (to put text at the bottom)
\newcommand{\framefill}{\vskip0pt plus 1filll}

\algdef{SE}[SUBALG]{Indent}{EndIndent}{}{\algorithmicend\ }%
\algtext*{Indent}
\algtext*{EndIndent}


\title[Support Vector Machines]{SVM and their use in IR}
\date[May 2023]{May x, 2023}
\author[Lorenzo Zanolin]{
  Lorenzo Zanolin.
  \pdfnewline\texttt{lorenzo.zanolin@spes.uniud.it}
}
\institute{Department of Mathematics, University of Udine}

\begin{document}

\begin{frame}\titlepage\end{frame}

\begin{frame}{Outline}
\tableofcontents
\end{frame}

\section{SVM over linearly separable data}

\begin{frame}{Vector Space}
    
    \only<1->{Suppose you have to classify whether a document is \textit{relevant} or not. We can think to use \textit{terms} as features to divide properly the data. We will use a \textit{t-dimensional} vector space to represent our documents.}
    \uncover<2->{\begin{figure}[b]
        \includegraphics[scale=0.35]{graphics/vector-space.png}
    \end{figure}}
\end{frame}

\begin{frame}{Classification}
    \only<1->{For simplicity, we can reason using a 2D Space.\\ Data can be separated using a \textit{decision boundary}, which is an \textit{hyperplane}. }

    \only<1>{
        \begin{figure}[b]
            \includegraphics[scale=0.20]{graphics/before.png}
        \end{figure}}
        
    \hspace{-0.17em}
    
    \only<2>{
        \begin{figure}[b]
            \includegraphics[scale=0.20]{graphics/after.png}
        \end{figure}}

    \hspace{-0.17em}
    
\end{frame}


\begin{frame}{Support Vectors}
    \only<1->{In the previous example the \textit{decision boundary} is a line, represented by the equation~$a + b{t}_{1} + c{t}_{2} = 0$.\\  
    We can introduce two parallels hyperplanes (lines) to the decision boundary, called \textit{support vectors} whose equations are} 
    \only<1->{
    \begin{equation}
        \begin{cases}
          a + b{t}_{1} + c{t}_{2} = 1\\
          a + b{t}_{1} + c{t}_{2} = -1\\
        \end{cases}\
    \end{equation}}

    \uncover<2->{
        \begin{figure}[]
            \includegraphics[scale=0.20]{graphics/support vectors.jpg}
        \end{figure}}
    \hspace{-0.1em}
    
\end{frame}

\begin{frame}{Margin}
    \only<1->{For convenience, we can rewrite equations using \textit{vectorization} notation.  
    \begin{equation}
        \begin{cases}
          {b}^{T}t + a = 0\\
          {b}^{T}t + a = \pm1\\
        \end{cases}\
    \end{equation}}
    
    \uncover<2->{Intuitively, we can define the \textit{margin} of the two support vectors as the distance between them.\\}
    \vspace{5mm} %5mm vertical space
    \uncover<3->{We can consider two points $x_1,x_2$ that lie respectively on the two support vectors, their distance is~$\lambda\lVert b \rVert = \frac{2}{\sqrt{{b}^{T}b}}$.\\ 
    \vspace{5mm} %5mm vertical space
    SVM are used to find the \textit{maximum margin linear classifier}, thus we want to maximize the margin.}
     
\end{frame}

\begin{frame}{Cost Function}
    \only<1->{Remembering we want to classify documents, our goal is to find specific $b$ s.t.
    given a document $x$ belonging to class $y$ the decision boundary behave the following:
    \begin{equation}
        \begin{cases}
          {b}^{T}x + a \geq 1\qquad if\ y = 1\\
          {b}^{T}x + a \leq -1\qquad if\ y = -1\\
        \end{cases}\
    \end{equation}}
    \uncover<2->{We can define the \textit{cost function} as a system of equations.
    \begin{equation}
        \begin{cases}
          \min_{b,a} \frac{\sqrt{{b}^{T}b}}{2}\\
          subject\ to \quad y_i(b^{T}x_i + a)\geq1 \quad \forall x_i\\
        \end{cases}\
    \end{equation}}
     
\end{frame}

\begin{frame}{Soft-Margin}
    \only<1->{In real world problem it is not likely to get an exactly separate line dividing the data within the space. It would be better for the smooth boundary to ignore few data points than be curved or go in loops, around the outliers.\\}
    \vspace{5mm}
    \uncover<2->{So, we will use \textit{slack variables} to introduce a penalty for each misclassified point.\\}
    \vspace{5mm}
    \uncover<3>{The new cost function will be
        \begin{equation}
            \begin{cases}
              \min_{b,a} \frac{\sqrt{{b}^{T}b}}{2} + C  \sum_{i} \xi_i\\
              subject\ to \quad y_i(b^{T}x_i + a)\geq1-\xi_i\quad and\ \xi_i>0 \quad \forall x_i\\
            \end{cases}\
        \end{equation}}
\end{frame}

\begin{frame}{Visualization}
    \only<1->{The larger is C the stricter the classification is, since a larger C will give more evidence to slack variables.
        \begin{equation}
                \begin{cases}
                  \min_{b,a} \frac{\sqrt{{b}^{T}b}}{2} + C  \sum_{i} \xi_i\\
                  subject\ to \quad y_i(b^{T}x_i + a)\geq1-\xi_i\quad and\ \xi_i>0 \quad \forall x_i\\
                \end{cases}\
            \end{equation}
    }
    \uncover<2->{
        \begin{figure}[b]
            \includegraphics[scale=0.055]{graphics/slack.jpg}
        \end{figure}
    }
\end{frame}

\section{SVM over non linearly separable data}

\begin{frame}{Non linearly separable data}
    \only<1->{What if our data is not linearly separable?}
    \uncover<2->{
     It should be better to reason in a bigger dimensional space.
     \begin{figure}[]
            \includegraphics[scale=0.4]{graphics/kernel.jpg}
        \end{figure}
    }
    \uncover<3>{
    But augmenting dimensions costs a lot\ldots
    }
\end{frame}

\begin{frame}{Kernel Trick}
    
    \uncover<1->{Consider the function $\phi : \mathbb{R}^3 \mapsto \mathbb{R}^{10}$ used to map points in a new vector space. Calculating the \textit{similarity} ${\phi(x_i)}^T\phi(x_j)$ between each point may be intractable.\\}
    \vspace{5mm}
    \uncover<2->{Here \textit{Kernel Trick} comes handy.\\}
    \vspace{5mm}
    \uncover<3->{It consists of a simple linear algebra reformulation, 
        \begin{equation}
            {\phi(x_i)}^{T}\phi(x_j) = K(x_i,x_j) = {(1+{x_i}^{T}x_j)}^2.
        \end{equation}\\
    }
    
    \uncover<4->{Instead of doing the complex computations in the 10-dimensional space, we reach the same result within the 3-dimensional space by calculating the dot product.}


    \vspace{5mm}
    
\end{frame}

\begin{frame}{Gaussian Kernel}
    \only<1->{There are lots of Kernel functions, an example is the \textit{Gaussian}.\\}
    \vspace{5mm}
    \uncover<2->{The idea behind is to use all $n$ training points as \textit{landmarks}.\\ Then we can calculate the similarities of each point and all the landmarks.\\}
    \uncover<3->{\begin{equation}
        \forall l_i\ similarity(x,l^{(i)}) = \exp(-\frac{{\lVert x - l^{(i)} \rVert}^2}{2\sigma^{2}})
    \end{equation}\\}
    \vspace{5mm}
    \uncover<4>{The result will be a mapping for each point in a n-dimensional space. Finally, we can identify an hyperplane which can divide correctly the two classes of points.}

\end{frame}

\section{SVM use in IR}
\begin{frame}{Hybrid use}
    \only<1->{An hybrid technique used in the IR field will be presented; it uses two components: 
    \begin{itemize}
        \item \textbf{K-Means}: model for \textit{unsupervised classification};
        \item \textbf{SVM}: model for \textit{supervised classification}.
    \end{itemize}
    }
    \vspace{5mm}
    \uncover<2->{K-means algorithm is one of the most used clustering algorithms. It consists of partitioning unlabeled objects into k classes, where k is no predefined.\\ A brief explanation will follow.}
\end{frame}

\begin{frame}{K-means}
    \only<1->{The aim is to create clusters that contain similar documents. Given a training set ${x^{(1)},x^{(2)},\ldots,x^{(m)}}$, the algorithm used is the following:\\}
    \uncover<2->{
        \begin{algorithm}[H]\small
            \caption{K-Means}
            \begin{algorithmic}[1]
              \State $K\gets randomValue$  
              \Procedure{K-Means}{$K$}\Comment{}
                \State \texttt{initialize\ K\ centroids in random positions} 
                \State \textbf{loop\{}
                    \Indent
                    \For{$i \gets 1$ to $m$}
                        \State \texttt{Calculate the closest centroid to $x^{(i)}$}
                    \EndFor
                    
                    \For{$k \gets 1$ to $K$}
                        \State \texttt{Calculate the new centroid of cluster $k$}
                    \EndFor
                    \EndIndent
                \State \textbf{ \} }
              \EndProcedure
            \end{algorithmic}
          \end{algorithm}
    }
\end{frame}

\begin{frame}{Visual explanation}
    
\end{frame}

\end{document}